{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "celltoolbar": "Raw Cell Format",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Chapter-1-HW",
      "provenance": [],
      "include_colab_link": true
    }
  },

  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/catawba-data-mining/CIS-3902-Data-Mining/blob/main/Chapter11_Homework_4_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
       ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfKm7jpAYc0z"
      },
      "source": [
    "Catawba College Data Mining Class Data 100: Chapter 11 Case Study: Berkeley Policing\n",
    "\n",
    "INSTRUCTIONS: To open this file in Google COLAB, click on the COLAB link (blue). Follow all instructions in the Program once in Colab. You do not have to turn in the code. After studying the code and the output with your group, return to Blackboard and complete the rest of the activity.\n",
    "\n",
    "In this notebook, we will clean a dataset, and then use it for some basic exploration. \n",
    "\n",
    "We must begin by installing some necessary packages. \n",
    "\n",
    "STEP 1: Place your cursor (click) in the code cells and click on the triangle to the left of the code to execute (click RUN ANYWAY on first code block if you get an authorization error). Some code blocks WILL NOT display any output. Some code blocks generate many messages! You can clear these by clicking on the x where the messages are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-creation",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#STEP 1:  we need to install datascience first because it is not a typical package that comes with our programming environment\n",
    "#more information can be found here (optional reading https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/)\n",
    "#!pip install datascience\n",
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install datascience\n",
    "!{sys.executable} -m pip install sodapy\n",
    "!{sys.executable} -m pip install seaborn\n",
    "#\n",
    "#after this is executed you can click on the x (person changes to x when cursor is hovered) in order to clear messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-intensity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many of these import statements are repeated across projects.\n",
    "from datascience import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sodapy import Socrata\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plots\n",
    "plots.style.use('fivethirtyeight')\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-isolation",
   "metadata": {},
   "source": [
    "We begin by obtaining our copy of the Calls dataset. We are going to use the Socrata API to download the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-hypothesis",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This uses the Socrata API to download a copy of the dataset. \n",
    "\n",
    "# Unauthenticated client only works with public data sets. Note 'None'\n",
    "# in place of application token, and no username or password:\n",
    "client = Socrata(\"data.cityofberkeley.info\", None)\n",
    "\n",
    "# Results returned as JSON from API / converted to Python list of\n",
    "# dictionaries by sodapy.\n",
    "results = client.get(\"k2nh-s5h5\", limit=2000)\n",
    "# This restricts us to 2000 records,\n",
    "# but we could change this number for a larger value\n",
    "\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "results = pd.DataFrame.from_records(results)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-participant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We must remove columns that we don't wish to use. \n",
    "# In this case, there are some geographic variables we can discard immediately. \n",
    "calls = results[['caseno', 'offense', 'eventdt', 'eventtm', 'cvlegend', 'cvdow', 'indbdate', 'block_location', 'blkaddr', 'city', 'state']]\n",
    "calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-washer",
   "metadata": {},
   "source": [
    "Now that we have obtained a copy of the dataset, and stored it in a dataframe, it is time to examine the dataset for potential problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-footwear",
   "metadata": {},
   "source": [
    "We will begin by looking for columns with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True if row contains at least one null value\n",
    "null_rows = calls.isnull().any(axis=1)\n",
    "calls[null_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-brave",
   "metadata": {},
   "source": [
    "A small number of calls don't have values listed for block address (blkaddr). We can make assumptions about what these values might be, but we must remember that these are assumptions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-hands",
   "metadata": {},
   "source": [
    "Another interesting note, is that the event date (eventdt) lists all times as midnight, but the exact time is in event time (eventtm). We can write a function that manipulates the strings to make a new column that mergest the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_event_datetimes(calls):\n",
    "    combined = pd.to_datetime(\n",
    "        # Combine date and time strings\n",
    "        calls['eventdt'].str[:10] + ' ' + calls['eventtm'],\n",
    "        infer_datetime_format=True,\n",
    "    )\n",
    "    return calls.assign(eventdttm=combined)\n",
    "\n",
    "# To peek at the result without mutating the calls DF:\n",
    "calls.pipe(combine_event_datetimes).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the calls dataframe is unaltered. \n",
    "calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-firewall",
   "metadata": {},
   "source": [
    "It is also useful to check and see which columns were human input. One way of doing that is by checking for unique values. Data input by humans requires special consideration, to check for problems such as spelling errors. \n",
    "\n",
    "We are going to look more closely at two columns, Offense, which stores the offense type, and CVLegend, which stores the event description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-latter",
   "metadata": {},
   "outputs": [],
   "source": [
    "calls['offense'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-handle",
   "metadata": {},
   "outputs": [],
   "source": [
    "calls['cvlegend'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-curve",
   "metadata": {},
   "source": [
    "Lastly, we can make a small dataframe to allow cvdow to be matched to a specific day of the week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-consumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_of_week = pd.DataFrame([['0', 'Sunday'], ['1', 'Monday'], ['2', 'Tuesday'], ['3', 'Wednesday'], ['4', 'Thursday'], \n",
    "                            ['5', 'Friday'], ['6', 'Saturday']], \n",
    "                           columns=['cvdow', 'day'])\n",
    "day_of_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_weekday(calls):\n",
    "    return calls.merge(day_of_week, on='cvdow')\n",
    "calls.pipe(match_weekday).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also drop columns we do not need. \n",
    "def drop_unneeded_cols(calls):\n",
    "    return calls.drop(columns=['cvdow', 'indbdate', 'block_location', 'city',\n",
    "                               'state', 'eventdt', 'eventtm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-julian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lastly we pipe the dataset through the functions to get our final version\n",
    "calls_final = (calls.pipe(combine_event_datetimes)\n",
    "               .pipe(match_weekday)\n",
    "               .pipe(drop_unneeded_cols))\n",
    "calls_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-listing",
   "metadata": {},
   "source": [
    "To conclude, you are going to do a bit of exploratory analysis and visualization on this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's your turn now. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-gender",
   "metadata": {},
   "source": [
    "First, using what you learned in chapter 10, can you make a bar chart comparing the number of cases on different days of the week?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place the code for the bar chart here:\n",
    "# Sets the order for the days\n",
    "day_order = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "# Resizes the plot so that it will be large enough to read easily\n",
    "plots.figure(figsize=(16, 10))\n",
    "# Add the plot here. Use a seaborn count plot, and set the x value to day, the order to day_order and the data to calls_final. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets use value counts to see what the most frequent offenses are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly, make a function similar to the one used in homework 3 to determine the frequency of crimes per day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then, using that function, make dataframes for each day of the week. \n",
    "# Sunday Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-forwarding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monday Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuesday Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wednesday Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-dinner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thursday Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Friday Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-struggle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saturday Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, make a seaborn count plot that answers this question:\n",
    "# What is the most frequent crime on the day of the week with the most crime?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-pilot",
   "metadata": {},
   "source": [
    "When you finish, download your ipynb file and upload it to blackboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
